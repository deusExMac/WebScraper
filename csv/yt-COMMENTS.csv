"dateaccessed";"url";"videoComments"
"30/07/2022 19:00:08";"https://www.youtube.com/watch?v=fkf4IBRSeEc";"['SJ H\n1 year ago\nProf. Brunton always delivers the best explanations on the subjects! His videos really help me a lot! Kudos!\nShow less\nRead more\n4\n4\nReply', 'Prof. Brunton always delivers the best explanations on the subjects! His videos really help me a lot! Kudos!\nShow less\nRead more', 'Shashidhar Muniswamy\n4 months ago\nThank you, Prof. Brunton. I have a question: supposing I have done this series of experiments with a target measure that cannot be categorized but is a continuous value, then can I use PCA?\nShow less\nRead more\n0\n0\nReply', 'Thank you, Prof. Brunton. I have a question: supposing I have done this series of experiments with a target measure that cannot be categorized but is a continuous value, then can I use PCA?\nShow less\nRead more', 'Gianluca\n1 year ago (edited)\nThanks for this video! What\'s the difference between cumulative sum of sigma instead of lamba? (11:30) In ""Principal Component Analysis (PCA) 2 [Python]"" you do the cumulative sum of S (sigma elements). Thanks!\nShow less\nRead more\n0\n0\nReply', 'Thanks for this video! What\'s the difference between cumulative sum of sigma instead of lamba? (11:30) In ""Principal Component Analysis (PCA) 2 [Python]"" you do the cumulative sum of S (sigma elements). Thanks!\nShow less\nRead more', 'Aayush patel\n1 year ago\nThese videos are the PCA for data driven engineering!!Thank you for bringing up these series publicly!!\nShow less\nRead more\n2\n2\nReply', 'These videos are the PCA for data driven engineering!!Thank you for bringing up these series publicly!!\nShow less\nRead more', ""hrtlsbstrd\n1 year ago\nSteve love your work, and your production values are fantastic - fyi, you can likely fix a lot of the contact noises from your lav mic with some relatively simple filtering and processing, and lowering the levels a bit during the recording could help avoid the distortion that crops up sometimes. If you'd like to discuss, let me know and I'll get you my contact details.\nShow less\nRead more\n0\n0\n❤ by Steve Brunton\nReply"", ""Steve love your work, and your production values are fantastic - fyi, you can likely fix a lot of the contact noises from your lav mic with some relatively simple filtering and processing, and lowering the levels a bit during the recording could help avoid the distortion that crops up sometimes. If you'd like to discuss, let me know and I'll get you my contact details.\nShow less\nRead more"", ""rb3.1415\n1 year ago (edited)\nWhat the hell. One can download your book for free?! You sir are a saint. I will work thru it and if I like it I will definitely purchase it!! (I'm pretty sure I will like it, because I like all your videos so far) PS: I am so proud of you guys. You are bringing humanity forward with content like this being free. I encourage everyone who can to purchase content from sources like this\nShow less\nRead more\n6\n6\nReply"", ""What the hell. One can download your book for free?! You sir are a saint. I will work thru it and if I like it I will definitely purchase it!! (I'm pretty sure I will like it, because I like all your videos so far) PS: I am so proud of you guys. You are bringing humanity forward with content like this being free. I encourage everyone who can to purchase content from sources like this\nShow less\nRead more"", 'Vijanth Asirvadam\n2 years ago\nSteve, able to explain PCA from classical statistiscal point of view. Very clear\nShow less\nRead more\n1\n1\nReply', 'Steve, able to explain PCA from classical statistiscal point of view. Very clear\nShow less\nRead more', 'Spin Orb\n1 year ago\nI logged in just for this, which I almost never do xD I wanted to say: Thank you! Your video series is great, enjoyful, and helps getting familiar with the topic rapidly. The same applies to the book, which you link at for free. Thank you.\nShow less\nRead more\n6\n6\n❤ by Steve Brunton\nReply', 'I logged in just for this, which I almost never do xD I wanted to say: Thank you! Your video series is great, enjoyful, and helps getting familiar with the topic rapidly. The same applies to the book, which you link at for free. Thank you.\nShow less\nRead more', 'Critical Cog\n1 year ago\nWow, excellent explanation. Thank you so much.\nShow less\nRead more\n0\n0\nReply', 'Wow, excellent explanation. Thank you so much.\nShow less\nRead more', 'Tymothy Lim\n1 year ago\nThank you very much for this video! Learnt quite a bit from this :)\nShow less\nRead more\n0\n0\nReply', 'Thank you very much for this video! Learnt quite a bit from this :)\nShow less\nRead more', 'Lucas S. Lovén\n5 months ago\nLove this series! Just bought your book\nShow less\nRead more\n0\n0\nReply', 'Love this series! Just bought your book\nShow less\nRead more', 'Hector Ponce\n1 year ago\nDoes the concept of cross-loadings exist in PCA like it does in EFA? If it does exist, what are the criteria to determine so?\nShow less\nRead more\n0\n0\nReply', 'Does the concept of cross-loadings exist in PCA like it does in EFA? If it does exist, what are the criteria to determine so?\nShow less\nRead more', 'Vince Caulfield\n2 years ago\nHi Steve, There may be a tiny typo in Page#22 in your Data Driven Science book. The equation(1.26) is supposed to be $B = X - \\bar X$ to represent demeaned data $X$ while it shows $B = X - \\bar B$ on the book. Please correct me if I am wrong.\nShow less\nRead more\n4\n4\nReply', 'Hi Steve, There may be a tiny typo in Page#22 in your Data Driven Science book. The equation(1.26) is supposed to be $B = X - \\bar X$ to represent demeaned data $X$ while it shows $B = X - \\bar B$ on the book. Please correct me if I am wrong.\nShow less\nRead more', 'spyhunter0066\n7 months ago\nCan you also show how to get covariance matrix from a Gaussian function results from its fit on a Gaussian looking data. Any suggestion for a book to explain this kind of stuff? Cheers.\nShow less\nRead more\n0\n0\nReply', 'Can you also show how to get covariance matrix from a Gaussian function results from its fit on a Gaussian looking data. Any suggestion for a book to explain this kind of stuff? Cheers.\nShow less\nRead more', 'Bozlul Karim\n1 year ago\nI like your explanation. Please check equation 1.26 on your databook.\nShow less\nRead more\n1\n1\nReply', 'I like your explanation. Please check equation 1.26 on your databook.\nShow less\nRead more', 'lgn\n1 year ago\nsuperb explanation. Thank you!\nShow less\nRead more\n0\n0\nReply', 'superb explanation. Thank you!\nShow less\nRead more', ""Noah Barrow\n6 months ago\nHey Dr. Brunton. Awesome video yet again! I've been snooping around kaggle, and found a dataset on body performance given a host of variables. I thought i'd try using PCA to determine the most influential characteristics within the data and began working with it in matlab. I was able to get tons of outputs (a thrill unto itself) and a nice little scatter plot! However, when all was said and done I had difficulty understanding which variables were most influential by looking at the scatter plot and PCA breakdown. What should I be doing/thinking to gain that intuition? Thanks!\nShow less\nRead more\n1\n1\nReply"", ""Hey Dr. Brunton. Awesome video yet again! I've been snooping around kaggle, and found a dataset on body performance given a host of variables. I thought i'd try using PCA to determine the most influential characteristics within the data and began working with it in matlab. I was able to get tons of outputs (a thrill unto itself) and a nice little scatter plot! However, when all was said and done I had difficulty understanding which variables were most influential by looking at the scatter plot and PCA breakdown. What should I be doing/thinking to gain that intuition? Thanks!\nShow less\nRead more"", 'kyub\n1 year ago\nI started watching from SVD til here and it was super helpful! Thank you so so much.\nShow less\nRead more\n0\n0\nReply', 'I started watching from SVD til here and it was super helpful! Thank you so so much.\nShow less\nRead more', ""Resa P.\n1 year ago\nI've watched a lot of PCA videos and this is really the best one. You're amazing!\nShow less\nRead more\n12\n12\n❤ by Steve Brunton\nReply"", ""I've watched a lot of PCA videos and this is really the best one. You're amazing!\nShow less\nRead more"", 'Truthspiller\n1 year ago\nHello, can you show with examples how to curvilinear component analysis?\nShow less\nRead more\n0\n0\nReply', 'Hello, can you show with examples how to curvilinear component analysis?\nShow less\nRead more', 'Daniel Zhang\n2 years ago\nSo far this is the best video of PCA explanation.\nShow less\nRead more\n64\n64\nReply', 'So far this is the best video of PCA explanation.\nShow less\nRead more', 'Miliano Alvez\n2 years ago\nExcelent teaching. I have one question tho. When you wrote the covariance matrix of the rows (6:00) because each row is a measurement vector I thought its the covariance between the measurements but then you wrote C=(BT)(B) which is the covariance of the features. Can you explain please.\nShow less\nRead more\n7\n7\nReply', 'Excelent teaching. I have one question tho. When you wrote the covariance matrix of the rows (6:00) because each row is a measurement vector I thought its the covariance between the measurements but then you wrote C=(BT)(B) which is the covariance of the features. Can you explain please.\nShow less\nRead more', 'Qamar Kilani\n2 years ago\nThank You Steve for your Create videos , can you please add sequence number for each lecture to keep tracking\nShow less\nRead more\n1\n1\n❤ by Steve Brunton\nReply', 'Thank You Steve for your Create videos , can you please add sequence number for each lecture to keep tracking\nShow less\nRead more', 'Jacob Anderson\n2 years ago\nDo you have a patreon? How can I help support this content? Just these materials on Ch1 and 2 have been amazing. Will it extend to addiitonal chapters?\nShow less\nRead more\n17\n17\n❤ by Steve Brunton\nReply', 'Do you have a patreon? How can I help support this content? Just these materials on Ch1 and 2 have been amazing. Will it extend to addiitonal chapters?\nShow less\nRead more', '淼淼\n1 year ago\nI am a phd student learning inverse scattering, your lectures help me with understanding those concept :) greetings from naples\nShow less\nRead more\n0\n0\nReply', 'I am a phd student learning inverse scattering, your lectures help me with understanding those concept :) greetings from naples\nShow less\nRead more', ""Reginald Robinson\n1 year ago\nCan you please share what software and equipment you're using for this presentation?\nShow less\nRead more\n0\n0\nReply"", ""Can you please share what software and equipment you're using for this presentation?\nShow less\nRead more"", 'Edha S\n1 year ago\nThank you so much. You made it really easy to understand.\nShow less\nRead more\n2\n2\n❤ by Steve Brunton\nReply', 'Thank you so much. You made it really easy to understand.\nShow less\nRead more', '0isNaN\n2 years ago (edited)\nThis is the right amount of math to make sense in such a condensed video. PS: The covariance matrix is missing the 1/n term which is in the book and the code in later video.\nShow less\nRead more\n7\n7\n❤ by Steve Brunton\nReply', 'This is the right amount of math to make sense in such a condensed video. PS: The covariance matrix is missing the 1/n term which is in the book and the code in later video.\nShow less\nRead more', 'YX\n1 year ago\nAmazing video. To the point and efficient.\nShow less\nRead more\n2\n2\n❤ by Steve Brunton\nReply', 'Amazing video. To the point and efficient.\nShow less\nRead more', 'M CC\n2 years ago\nVery clear, excellent\nShow less\nRead more\n1\n1\n❤ by Steve Brunton\nReply', 'Very clear, excellent\nShow less\nRead more', 'Coughaholic\n2 years ago\nThank you for the lecture, its been very helpful. On an unrelated note, how do you write backwards with such ease?\nShow less\nRead more\n48\n48\n❤ by Steve Brunton\nReply', 'Thank you for the lecture, its been very helpful. On an unrelated note, how do you write backwards with such ease?\nShow less\nRead more', 'Le Li\n10 months ago\nhi doctor，really usefull to watch your lecture,but in the video,you have pointed out that T matrix is the principle components, however ,this is what confused me, my knowlage is that the col vector of loading are principle components, T is just transformed version of the data B. pls correct me if im wrong, thanks.\nShow less\nRead more\n0\n0\nReply', 'hi doctor，really usefull to watch your lecture,but in the video,you have pointed out that T matrix is the principle components, however ,this is what confused me, my knowlage is that the col vector of loading are principle components, T is just transformed version of the data B. pls correct me if im wrong, thanks.\nShow less\nRead more', 'cy aaronk\n1 year ago\nAmazing! Thank you!\nShow less\nRead more\n0\n0\nReply', 'Amazing! Thank you!\nShow less\nRead more', 'T4l0nITA\n2 years ago (edited)\nThe best video on PCA I could find on youtube, no messy blackboards, jokes or oversimplification, just solid explanation, great job.\nShow less\nRead more\n16\n16\n❤ by Steve Brunton\nReply', 'The best video on PCA I could find on youtube, no messy blackboards, jokes or oversimplification, just solid explanation, great job.\nShow less\nRead more', 'Eduardo Antonio Roque Díaz\n1 year ago\nIf I have outliers in my dataset, can this affect the PCA?, because I have tried with cases of this type and it usually identifies a single principal component\nShow less\nRead more\n0\n0\nReply', 'If I have outliers in my dataset, can this affect the PCA?, because I have tried with cases of this type and it usually identifies a single principal component\nShow less\nRead more', 'saalim zafar\n2 weeks ago\nIntroduction is one thing, presentation is another. One who combines both gets all the attention!!\nShow less\nRead more\n0\n0\nReply', 'Introduction is one thing, presentation is another. One who combines both gets all the attention!!\nShow less\nRead more', 'S M\n10 months ago (edited)\nIt can be helpful to use the names ""features"" (to refer to the \'n\' different pixels in a photo, or the \'n\' different characteristics of rats which may predict cancer) and ""snapshots"" (to refer to the \'m\' different measurements (e.g. people\'s photos, or rats)). Then, it doesn\'t matter whether you have the ""features"" as columns or rows – Corr(feat) = feature-wise correlation matrix, where entries represent the correlation between two features, and the eigenvectors of this matrix are the ""eigenfeatures"". If you happen to have ""features"" as columns, then Corr(feat) = [X][X^T]. If you happen to have the ""features"" as rows, then Corr(feat) = [X^T][X]. Similarly, for the ""snapshots"" we have the Corr(snap) = snapshot-wise correlation matrix, where entries represent the correlation between two snapshots, and the eigenvectors of this matrix are the ""eigensnapshots"". Again, depending on whether the ""snapshots"" are in the rows or columns of X, you can find Corr(snap). This also helps when doing PCA, as you generally wish to reduce the number of ""features"", and are therefore interested in determining the eigenvectors of Corr(feat). No need to sweat over how your data is organized in the matrix X, or any annoying conventions for PCA. In short, it is easier to think of ""features & snapshots"" than ""rows & columns"".\nShow less\nRead more\n0\n0\nReply', 'It can be helpful to use the names ""features"" (to refer to the \'n\' different pixels in a photo, or the \'n\' different characteristics of rats which may predict cancer) and ""snapshots"" (to refer to the \'m\' different measurements (e.g. people\'s photos, or rats)). Then, it doesn\'t matter whether you have the ""features"" as columns or rows – Corr(feat) = feature-wise correlation matrix, where entries represent the correlation between two features, and the eigenvectors of this matrix are the ""eigenfeatures"". If you happen to have ""features"" as columns, then Corr(feat) = [X][X^T]. If you happen to have the ""features"" as rows, then Corr(feat) = [X^T][X]. Similarly, for the ""snapshots"" we have the Corr(snap) = snapshot-wise correlation matrix, where entries represent the correlation between two snapshots, and the eigenvectors of this matrix are the ""eigensnapshots"". Again, depending on whether the ""snapshots"" are in the rows or columns of X, you can find Corr(snap). This also helps when doing PCA, as you generally wish to reduce the number of ""features"", and are therefore interested in determining the eigenvectors of Corr(feat). No need to sweat over how your data is organized in the matrix X, or any annoying conventions for PCA. In short, it is easier to think of ""features & snapshots"" than ""rows & columns"".\nShow less\nRead more', ""Fábio Felix\n2 months ago\nHi professor, Just one question. If your X matrix has samples in the rows and sample features in the columns, then the correct shouldn't be to calculate the column-means(X), instead of row-means(X), and subtract each column-value by its respective column-mean? So, each X column (feature) has mean = 0.\nShow less\nRead more\n0\n0\nReply"", ""Hi professor, Just one question. If your X matrix has samples in the rows and sample features in the columns, then the correct shouldn't be to calculate the column-means(X), instead of row-means(X), and subtract each column-value by its respective column-mean? So, each X column (feature) has mean = 0.\nShow less\nRead more"", 'Smsng2 Xprss\n3 weeks ago\n4:45 here you’re summing over the elements of each row, but in the book on page 21 it say x_j = sum_i X_ij so you’re building the sum of each column. Is it a typo ?\nShow less\nRead more\n0\n0\nReply', '4:45 here you’re summing over the elements of each row, but in the book on page 21 it say x_j = sum_i X_ij so you’re building the sum of each column. Is it a typo ?\nShow less\nRead more', 'muhammad ali\n1 year ago\nExcellent ,connected ,simple\nShow less\nRead more\n0\n0\nReply', 'Excellent ,connected ,simple\nShow less\nRead more', 'Brayden St.Pierre\n1 year ago\nThanks for the great explanation! In your next video, can you please explain how you are writing backward!?\nShow less\nRead more\n6\n6\nReply', 'Thanks for the great explanation! In your next video, can you please explain how you are writing backward!?\nShow less\nRead more', ""nami\n8 months ago\nWouldn't we need to divide by N or N-1 for the covariance matrix? I know covariance as sij = 1/(N-1)* sum (n=>N) (v_in*v_jn)\nShow less\nRead more\n0\n0\nReply"", ""Wouldn't we need to divide by N or N-1 for the covariance matrix? I know covariance as sij = 1/(N-1)* sum (n=>N) (v_in*v_jn)\nShow less\nRead more"", 'Time Series SCC\n2 years ago\nPlease could you make a video about singular spectrum analysis?\nShow less\nRead more\n0\n0\nReply', 'Please could you make a video about singular spectrum analysis?\nShow less\nRead more', 'Mohamed Emara\n2 years ago\nThis is amazing!\nShow less\nRead more\n3\n3\n❤ by Steve Brunton\nReply', 'This is amazing!\nShow less\nRead more', ""Alexander Feng\n1 year ago\nI believe that there's a typo. The principal components are the columns of V.\nShow less\nRead more\n4\n4\nReply"", ""I believe that there's a typo. The principal components are the columns of V.\nShow less\nRead more"", 'harish p\n2 years ago\nHi sir, The approach of explanation is good but the clarity of the main mathematical concept (eigenvalue and eigenvectors) lags. Thanks for sharing this awesome content. Love and respect from India.\nShow less\nRead more\n1\n1\n❤ by Steve Brunton\nReply', 'Hi sir, The approach of explanation is good but the clarity of the main mathematical concept (eigenvalue and eigenvectors) lags. Thanks for sharing this awesome content. Love and respect from India.\nShow less\nRead more', 'Minjie Shen\n2 years ago\nIf we do row-wise correlation with respect to B, should it be C=B * B_T instead of B_T * B?\nShow less\nRead more\n5\n5\nReply', 'If we do row-wise correlation with respect to B, should it be C=B * B_T instead of B_T * B?\nShow less\nRead more', 'Marco M\n2 years ago\nAwesome! Thank you!\nShow less\nRead more\n3\n3\n❤ by Steve Brunton\nReply', 'Awesome! Thank you!\nShow less\nRead more', ""Jaanus Kiipli\n1 year ago\nCorrect me if I'm wrong, but B transposed multiplied by B sums up the products of mean centered values, but to get the covariation we still need to divide by number of rows in X as covariation is defined as E{(X-E(X))*(Y-E(Y))} not just sum of (X-E(X))*(Y-E(Y)) over measurements\nShow less\nRead more\n0\n0\nReply"", ""Correct me if I'm wrong, but B transposed multiplied by B sums up the products of mean centered values, but to get the covariation we still need to divide by number of rows in X as covariation is defined as E{(X-E(X))*(Y-E(Y))} not just sum of (X-E(X))*(Y-E(Y)) over measurements\nShow less\nRead more"", 'fitriani nasir\n8 months ago\nthank u so much for this informative video, sir. I have a question, what does negative & positive value means in the final result of PCA? For example, i have my final PCA result with total data 156 rows as follow: --------------------------------------- No. | PC1 | PC2 | PC3 --------------------------------------- 0 | -0.43 | 0.2 | 0.3 1 | 0.23 | -0.3 | 0.2 2 | -0.67 | -0.39 | -0.48 ...... 155 | 0.11 | 0.23 | -0.93\nShow less\nRead more\n0\n0\nReply', 'thank u so much for this informative video, sir. I have a question, what does negative & positive value means in the final result of PCA? For example, i have my final PCA result with total data 156 rows as follow: --------------------------------------- No. | PC1 | PC2 | PC3 --------------------------------------- 0 | -0.43 | 0.2 | 0.3 1 | 0.23 | -0.3 | 0.2 2 | -0.67 | -0.39 | -0.48 ...... 155 | 0.11 | 0.23 | -0.93\nShow less\nRead more', 'The Green Decoy\n2 years ago\nIf there was a Nobel Prize in Education (which there absolutely should be), then you should absolutely win.\nShow less\nRead more\n1\n1\nReply', 'If there was a Nobel Prize in Education (which there absolutely should be), then you should absolutely win.\nShow less\nRead more', 'Qasim Ahmad\n1 year ago\nIs it important to show 95% confidence ellipse in PCA? If my data is not drawing then what should i do ? can i used PCA score graph without 95% confidence ellipse?\nShow less\nRead more\n0\n0\nReply', 'Is it important to show 95% confidence ellipse in PCA? If my data is not drawing then what should i do ? can i used PCA score graph without 95% confidence ellipse?\nShow less\nRead more', 'pandu 123\n2 years ago\nthis is more than Awesome!! i want to ask you one question and it is here a1=[1,23,4,51,62,7,8,43,1,29] a2=[5,45,32,51,60,7,8,35,10,31] a3=[13,3,64,35,36,37,48,3,31,1] a4=[3,3,1,5,6,3,8,3,1,3] a5=[0,3,0,5,0,0,8,0,0,1] how can i figure out important columns (features) with eigenvalues and eigenvectors? As we can see here , importance of a4 and a5 is negligible! but how can i find out with this concept? I have eigenvalues and eigenvectors of this but do not know how to use them in this context ? after finding eigenvalues and eigenvectors , i know how to find PC.Because i have seen your videos . As i have seen in the comment section someone already asked this question . But i was not able to understand the Ans! kindly help me out.\nShow less\nRead more\n0\n0\nReply', 'this is more than Awesome!! i want to ask you one question and it is here a1=[1,23,4,51,62,7,8,43,1,29] a2=[5,45,32,51,60,7,8,35,10,31] a3=[13,3,64,35,36,37,48,3,31,1] a4=[3,3,1,5,6,3,8,3,1,3] a5=[0,3,0,5,0,0,8,0,0,1] how can i figure out important columns (features) with eigenvalues and eigenvectors? As we can see here , importance of a4 and a5 is negligible! but how can i find out with this concept? I have eigenvalues and eigenvectors of this but do not know how to use them in this context ? after finding eigenvalues and eigenvectors , i know how to find PC.Because i have seen your videos . As i have seen in the comment section someone already asked this question . But i was not able to understand the Ans! kindly help me out.\nShow less\nRead more', 'Richard LIN\n2 years ago\nBest explanation. Looking forward to video about Kernel PCA！\nShow less\nRead more\n3\n3\n❤ by Steve Brunton\nReply', 'Best explanation. Looking forward to video about Kernel PCA！\nShow less\nRead more', 'Sky\n6 months ago\nThis channel is amazing!\nShow less\nRead more\n0\n0\nReply', 'This channel is amazing!\nShow less\nRead more', ""U2 COLDPLAY\n1 year ago\nI am confused with SVD of B in step 4 , Isn't we do SVD or Eigen decomposition of C the covariance matrix? i.e. T=CV=UE, C=UEV' ? thank you\nShow less\nRead more\n0\n0\nReply"", ""I am confused with SVD of B in step 4 , Isn't we do SVD or Eigen decomposition of C the covariance matrix? i.e. T=CV=UE, C=UEV' ? thank you\nShow less\nRead more"", 'Kimberly G\n1 year ago\nSo as another way to look at this, are U the scores, sigma the eigenvalues, and V the loadings?\nShow less\nRead more\n0\n0\nReply', 'So as another way to look at this, are U the scores, sigma the eigenvalues, and V the loadings?\nShow less\nRead more', 'Radwa Elawadi\n1 year ago\nBest Illustration of PCA, I searched a lot for the explanation of equation till I find it in your video. I have outer question what are using in your presentation to appear on the screen and the board you use ?\nShow less\nRead more\n1\n1\nReply', 'Best Illustration of PCA, I searched a lot for the explanation of equation till I find it in your video. I have outer question what are using in your presentation to appear on the screen and the board you use ?\nShow less\nRead more', ""Rodrigo Maximo\n6 months ago\nAmazing video. I did the MIT lectures about Linear Algebra (that talked about SVD) and the Andrew Ng's ML course (that talked about PCA). This video was the perfect bridge to connect the two things in a coherent manner. Thank you very much, Dr. Brunton!\nShow less\nRead more\n0\n0\nReply"", ""Amazing video. I did the MIT lectures about Linear Algebra (that talked about SVD) and the Andrew Ng's ML course (that talked about PCA). This video was the perfect bridge to connect the two things in a coherent manner. Thank you very much, Dr. Brunton!\nShow less\nRead more"", 'Alheli Brito\n1 year ago\nWow, I saw n videos before this, beautiful explanation¡\nShow less\nRead more\n1\n1\nReply', 'Wow, I saw n videos before this, beautiful explanation¡\nShow less\nRead more', ""GamingShiiep\n4 months ago (edited)\n2:09 I just don't get it: Let's say we measured 1600 samples. Each sample measurement resulted in a concentration value for each of 26 Elements. How would that look like in the matrix? So my matrix would have 1600 rows and 26 columns, right?\nShow less\nRead more\n0\n0\nReply"", ""2:09 I just don't get it: Let's say we measured 1600 samples. Each sample measurement resulted in a concentration value for each of 26 Elements. How would that look like in the matrix? So my matrix would have 1600 rows and 26 columns, right?\nShow less\nRead more"", 'Peter Pam\n1 year ago\nIn step 3, the covariance matrix need to times (1/n) right?\nShow less\nRead more\n3\n3\nReply', 'In step 3, the covariance matrix need to times (1/n) right?\nShow less\nRead more', 'HaZe\n8 months ago\nCan someone explain to me, why covariance matrix is just the inner product of B transposed B?\nShow less\nRead more\n0\n0\nReply', 'Can someone explain to me, why covariance matrix is just the inner product of B transposed B?\nShow less\nRead more', ""Vinson Ciawandy\n7 months ago\nYou only said about the data should have 0 mean, but what about the standard deviation? Don't we need to scale the data first by dividing each measure by its standard deviation to make sure the PCA doesn't easily overfit to direction with the largest magnitude?\nShow less\nRead more\n0\n0\nReply"", ""You only said about the data should have 0 mean, but what about the standard deviation? Don't we need to scale the data first by dividing each measure by its standard deviation to make sure the PCA doesn't easily overfit to direction with the largest magnitude?\nShow less\nRead more"", 'Vijanth Asirvadam\n1 year ago\nThe alst part of the video on how SVD and PCA are related really class of its own. IT show the expert should run video lectures\nShow less\nRead more\n0\n0\nReply', 'The alst part of the video on how SVD and PCA are related really class of its own. IT show the expert should run video lectures\nShow less\nRead more', 'RC Link\n1 year ago\nThe data matrix is a wide matrix, so if it is already zero mean, then in this case the PC XV is equal to XU (Considering U from the SVD lecture)?\nShow less\nRead more\n0\n0\nReply', 'The data matrix is a wide matrix, so if it is already zero mean, then in this case the PC XV is equal to XU (Considering U from the SVD lecture)?\nShow less\nRead more', 'Vinicius Viena\n1 year ago\ngood explanation in general. but you really should play with matrix dimension during explanations.\nShow less\nRead more\n0\n0\nReply', 'good explanation in general. but you really should play with matrix dimension during explanations.\nShow less\nRead more', 'IK LECTURES\n2 years ago\nNice lecture\nShow less\nRead more\n1\n1\n❤ by Steve Brunton\nReply', 'Nice lecture\nShow less\nRead more', ""Elad M\n2 years ago\ndoesn't C has to be B*B^T ? B^T * B is the covariance of the columns if I get this correctly\nShow less\nRead more\n4\n4\nReply"", ""doesn't C has to be B*B^T ? B^T * B is the covariance of the columns if I get this correctly\nShow less\nRead more"", 'jayasimha yenumaladoddi\n7 days ago\nCan you please make a video on OLPP?\nShow less\nRead more\n0\n0\nReply', 'Can you please make a video on OLPP?\nShow less\nRead more', 'Cathy Tang\n2 years ago\nIn the mean center part you are calculating row averages? As you described each row can be have ""sex, age, demographics, and so on"", these are not of the same category. Shouldn\'t it be column means?\nShow less\nRead more\n2\n2\n❤ by Steve Brunton\nReply', 'In the mean center part you are calculating row averages? As you described each row can be have ""sex, age, demographics, and so on"", these are not of the same category. Shouldn\'t it be column means?\nShow less\nRead more', 'Zhanfei Peng\n2 years ago (edited)\nStill confused how do we get BV=USigma since Vt doesn’t cancel with V right?\nShow less\nRead more\n1\n1\nReply', 'Still confused how do we get BV=USigma since Vt doesn’t cancel with V right?\nShow less\nRead more', ""Henry Yi\n9 months ago\nCan somebody explain to me why it's B'B not BB' since the data were stored row-wise.\nShow less\nRead more\n0\n0\nReply"", ""Can somebody explain to me why it's B'B not BB' since the data were stored row-wise.\nShow less\nRead more"", ""Luis Guanilo Quiñones\n1 year ago\nIt's a excellent explication, but the real question is , How have they recorded this video?\nShow less\nRead more\n1\n1\n❤ by Steve Brunton\nReply"", ""It's a excellent explication, but the real question is , How have they recorded this video?\nShow less\nRead more"", 'MrRynRules\n1 year ago\nThank you!\nShow less\nRead more\n0\n0\nReply', 'Thank you!\nShow less\nRead more', 'SandsnowStorm7\n9 months ago\nI came to learn about PCA, but now I’m just focusing on how he can write backwards so clearly.\nShow less\nRead more\n1\n1\nReply', 'I came to learn about PCA, but now I’m just focusing on how he can write backwards so clearly.\nShow less\nRead more', 'Thomas Wilke\n1 year ago\nIs this done with a glass whiteboard and the recording is mirrored?\nShow less\nRead more\n1\n1\nReply', 'Is this done with a glass whiteboard and the recording is mirrored?\nShow less\nRead more', 'Srikanth V\n1 year ago\nIf the images of X are not all independent, then X is not full rank matrix.then will we have only rank number of eigen faces?\nShow less\nRead more\n0\n0\nReply', 'If the images of X are not all independent, then X is not full rank matrix.then will we have only rank number of eigen faces?\nShow less\nRead more', 'MaeLSTRoM1997\n4 months ago\nIt took me a while to realize you are left handed and you just reflected the video so that what you write appears in the correct orientation for us. At first I was wondering if you managed to learn how to write backwards..\nShow less\nRead more\n1\n1\nReply', 'It took me a while to realize you are left handed and you just reflected the video so that what you write appears in the correct orientation for us. At first I was wondering if you managed to learn how to write backwards..\nShow less\nRead more', 'zsun01\n1 year ago\nBtB seems to calculate the cariance matrix of cols of B.\nShow less\nRead more\n1\n1\n❤ by Steve Brunton\nReply', 'BtB seems to calculate the cariance matrix of cols of B.\nShow less\nRead more']"
