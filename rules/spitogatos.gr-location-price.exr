

#
# IMPORTANT! Site identified that it is a bot.
#

{

# Description of the library

"libraryDescription": "Library to extract the team and manager name from english wikipedia articles on to premier league clubs",



# List of rule names, whose extracted data should be stored in the csv file. 
# IMPORTANT: date the url was accessed (dateaccessed) and the url are always automatically added to the csv file

"csvLineFormat":["propertyLocation", "propertyPrice"],


# How to fetch the urls

"renderPages":False,
"launchParameters" : { "executablePath":"F:\\ProgramFiles\\Programs\\Google\\Chrome\\Application\\chrome.exe", "userDataDir" : "C:\\Users\\Manolis\\AppData\\Local\\Google\\Chrome\\User Data" },



# http related stuff

"requestHeader": {

 "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
 "Accept-Encoding": "gzip, deflate, br",
 "Accept-Language": "en-US,en;q=0.5",
 "Connection": "keep-alive",
 "Sec-Fetch-Dest":"document",
 "Sec-Fetch-Mode":"navigate",
 "Sec-Fetch-Site":"none",
 "Sec-Fetch-User":"?1",
 "Upgrade-Insecure-Requests": "1"
},



"requestCookies": {

"AWSALB":"IqEeAmao0g78bCTyZT5F7Orxgeh5deoW7fN2rdQIHUEGVCTXR1OdllObV0QrComSJuHUSyHjunrIOnUd+TnDIMw/QLZE4TwnUbkkMqjATj2g0BEzcwAg7eD2pqlq", 
"AWSALBCORS":"IqEeAmao0g78bCTyZT5F7Orxgeh5deoW7fN2rdQIHUEGVCTXR1OdllObV0QrComSJuHUSyHjunrIOnUd+TnDIMw/QLZE4TwnUbkkMqjATj2g0BEzcwAg7eD2pqlq", 
"PHPSESSID":"l6tnr2el4mgb80a1h4kqhd8edd", 
"auth.strategy":"laravelJWT", 
"auth._token.laravelJWT":"false", 
"auth._token_expiration.laravelJWT":"false", 
"auth._refresh_token.laravelJWT":"false", 
"auth._refresh_token_expiration.laravelJWT":"false", 
"spitogatosHomepageMap":"0", 
"currentCurrency":"EUR", 
"openedTabs":"3", 
"reese84":"3:yZSmdxtiTYjRjsgNocEksA==:+qvpqaWonhKZ8p/JS3n6VhTMiPMmVn8Jn24jfiTEwgeC7P2Lx4Qa2vlxNgq/o9/iddBk5Yabyo6rnGFJZf6ZhziRtnKJCJreEfGXWGRHBRCUewffWfqsNV/0gFBTEbtl6caXWeOPGJI3X0WMXc+HhTtwGfMk/jnHsMzorozaPkct/DewhmqsN0G8TUYMHHAT0dG65roKJ4S/YXZ4RwZxj12S/4bOgdghZ4ebqFsUaMWaw9wO7YyormQuhR2niRlepdkBUuO7cmckaSoZKWDTX94V3KQJAVW7lLtkFH3gE+L54U/OgeKPDZqCbHzVviDcpuj145csss11DZlT3aqFJYfvLNq39OQprVoZUxrvvSYwnsoDVDqIV/t69IgIYBz4R6ztJvNaFIfyPgLChpqobPnePhGDkh3wlt8vVKsgJznWqwI4SZVQr1gZxT3Mk5WFBgRuu7QMGW8aZRd4tkuiktjyJE1oGN6iDarvNbtqLkc=:myANmAH+feBLSWM1hGbvlSZgK5h7yao198ffVhRqnks=", 
"euconsent-v2":"CPiEukAPiEukAAKAsAELCpCsAP_AAH_AAAyIJHtd_H__bW9r-f5_aft0eY1P9_r37uQzDhfNk-8F3L_W_LwX52E7NF36tq4KmR4ku1LBIUNlHMHUDUmwaokVryHsak2cpzNKJ7BEknMZOydYGF9vmxtj-QKY7_5_d3bx2D-t_9v239z3z81Xn3d53-_03LCdV5_9Dfn9fR_b89KP9_78v4v8_____3_e__3_7997-CRoBJhq3EAXZljgzaBhFAiBGFYSFUCgAgoBhaIDABwcFOysAn1hAgAQCgCMCIEOAKMCAQAACQBIRABIEWCAAAEQCAAEACIRCABgYBBYAWBgEAAIBoGKIUAAgSEGRARFKYEBUCQQGtlQglBdIaYQBVlgBQSI2KgARBICKwABAWDgGCJASsWCBJijfIARghQCiVCtRCAA.fngAAAAAAAAA", 
"addtl_consent":"1~39.4.3.9.6.9.13.6.4.15.9.5.2.7.4.1.7.1.3.2.10.3.5.4.21.4.6.9.7.10.2.9.2.18.7.6.14.5.20.6.5.1.3.1.11.29.4.14.4.5.3.10.6.2.9.6.6.9.4.4.29.4.5.3.1.6.2.2.17.1.17.10.9.1.8.6.2.8.3.4.146.8.42.15.1.14.3.1.8.10.25.3.7.25.5.18.9.7.41.2.4.18.21.3.4.2.7.6.5.2.14.18.7.3.2.2.8.20.8.8.6.3.10.4.20.2.13.4.6.4.11.1.3.22.16.2.6.8.2.4.11.6.5.33.11.8.1.10.28.12.1.3.21.2.7.6.1.9.30.17.4.9.15.8.7.3.6.6.7.2.4.1.7.12.13.22.13.2.12.2.10.1.4.15.2.4.9.4.5.4.7.13.5.15.4.13.4.14.10.15.2.5.6.2.2.1.2.14.7.4.8.2.9.10.18.12.13.2.18.1.1.3.1.1.9.25.4.1.19.8.4.5.3.5.4.8.4.2.2.2.14.2.13.4.2.6.9.6.3.4.3.5.2.3.6.10.11.6.3.16.3.11.3.1.2.3.9.19.11.15.3.10.7.6.4.3.4.6.3.3.3.3.1.1.1.6.11.3.1.1.11.6.1.10.5.2.6.3.2.2.4.3.2.2.7.15.7.14.1.3.3.4.5.4.3.2.2.5.3.1.1.1.2.9.1.6.9.1.5.2.1.7.10.11.1.3.1.1.2.1.3.2.6.1.12.5.3.1.3.1.1.2.2.7.7.1.4.1.2.6.1.2.1.1.3.1.1.4.1.1.2.1.8.1.7.4.3.2.1.3.5.3.9.6.1.15.10.28.1.2.2.12.3.4.1.6.3.4.7.1.3.1.1.3.1.5.3.1.3.4.1.1.4.2.1.2.1.2.2.2.4.2.1.2.2.2.4.1.1.1.2.2.1.1.1.1.2.1.1.1.2.2.1.1.2.1.2.1.7.1.2.1.1.1.2.1.1.1.1.2.1.1.3.2.1.1.8.1.1.6.2.1.6.5.1.1.1.1.1.2.2.3.1.1.4.1.1.2.2.1.1.4.3.1.2.2.1.2.1.2.3.1.1.2.4.1.1.1.5.1.3.6.3.1.5.2.3.4.1.2.3.1.4.2.1.2.2.2.1.1.1.1.1.1.11.1.3.1.1.2.2.5.2.3.3.5.1.1.1.4.2.1.1.2.5.1.9.4.1.1.3.1.7.1.4.5.1.7.2.1.1.1.2.1.1.1.4.2.1.12.1.1.3.1.2.2.3.1.2.1.1.1.2.1.1.2.1.1.1.1.2.4.1.5.1.2.4.3.8.2.2.9.7.2.2.1.2.1.4.6.1.1.6.1.1.2.6.3.1.2.201", 
"_oid":"926bfae9-2ee8-4aee-9f86-8e20819f9b2b", 
"_pbjs_userid_consent_data":"3524755945110770", 
"personalizedProperties":"true"
},


"requestUserAgent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:106.0) Gecko/20100101 Firefox/106.0",





# The list of ruleNames that must return non-empty values
# to consider the extraction successful and the data be written to the csv file. 

#"requiredFilledFields": ["propertyLocation", "propertyPrice"],

# Minimum percentage of ruleName(s) that must return non-empty
# data during their application in order to consider the extraction process
# a success and hence add the extracted data to the csv file.

"allowedMinimumFilled" : 0.8


# How should the downloaded html page be rendered.
# WebScraper supports two modes of URL downloads:
# static: meaning that the web page does not load its content dynamically (via js or ajax) and one http request is enough to get
#         the entire page content. 
# dynamic: meaning the athe web page has dynamic content that is loaded via js or ajax once the web page has
#          been downloaded or is scrolled. Example of such dynamic pages are e.g. youtube pages where comments are only displayed
#          when the user scrolls down. Scraping such dynamic pages is also supported by WebScraper. Dynamic pages load slower though.
# 
# If renderPages has a value of False, this means the no page rendering is carried out and should be used only in case of pages that
# do not load content dynamicaly.
# If renderPages has a value of True, this means the page rendering is done and should be used only in case of pages that load content
# dynamically. 
# 
# Since we access wikipedia articles with no dynamic content, we will statically load these pages. Hence renderPages is set to False. This
# will make page loading faster. If renderPages is missing, it defaults to False.

#"renderPages":False,



# List of individual rules comprising this library and that will be applied to each page downloaded. 
# NOTE: all rules mentioned below will ALWAYS be applied to a single page; this is done for all pages downloaded. 

"library": [

# Rule to extract/scrape the title of the downloaded article. The title
# is assumed to be the teams name, not it's full name (or formal name)

{

 # If no ruleName is specified, the extracted data of this rule cannot be used
 # in  the csvLineFormat attribute.
 
 "ruleName": "propertyLocation",
 
 
 # Short description
 
 "ruleDescription": "Extracts property price",
 
 
 
 # Regular expression specifying which URL pattern will trigger the
 # execution of this rule. 
 # This is a list ([]) meaning you may add many disjunctive regular expressions
 # Here we specify that this rule is to be activated when the URL contains en.wikipedia.org/wiki.
 # Special regular expression metacharacters (.) are escaped.
 
 "ruleURLActivationCondition": ["spitogatos\.gr/aggelia/\d+$"],
 
 
 # A CSS selector specifying the element on the page to scrape.
 # NOTE: the CSS selector may return more than one mathing element.
 
 "ruleCSSSelector": "span.property__address",
 
 
 # Once the CSS selector element in  ruleCSSSelector has been found, what 
 # exactly to extract from element: the text or some other attribute. 
 # text means simply return the text of the scraped element.
 "ruleTargetAttribute": "text",
 
 # Regular expression that specifies the condition the extracted text or attribute value has to
 # fulfill. Empty string here means no condition. If condition is not met, nothing is returned.
 "ruleContentCondition": "",
 
 # Does this rule return more than one result?
 "ruleReturnsMore": False,
 
 # If the rule returns more than one result, which result to return. Negative means all elements. 
 # This takes only effect if ruleReturnsMore is set to True.
 "ruleReturnedMatchPos": 0,
 
 # NOT YET SUPPORTED. How strict should the extraction be? If rule returns more than one result, should this be considered
 # an error?
 "ruleReturningMoreIsError": False,
 
 # List of characters to remove from the extracted value (text or attribute)
 "ruleRemoveChars": [],
},






# Rule to extract/scrape property price from pages related to a single
# property like e.g. https://www.spitogatos.gr/aggelia/2113156560 or
# https://www.spitogatos.gr/aggelia/1113221644

{

 
 
 "ruleName": "propertyPrice",
 
 
 # Short description
 "ruleDescription": "Extracts property price from pages related to a single property",
 
 # Pattern propery pages match
 "ruleURLActivationCondition": ["spitogatos\.gr/aggelia/\d+$"],
 
 
 "rulePreconditions" : [ 
                           # Check if this is about the price.
                           # NOTE: The price is on various parts of the same page; we
                           # get the value that's in the "Characteristics" (in greek)
                           # table only for testing purposes. Could get it easier
                           # by extracting other elements.
                           {
                              
                               "ecCSSSelector" : "dl.property__details dt:nth-child(1)", 
                               "ecTextCondition" : "(?i)(Τιμή)",                             
                               "ecRuleCSSSelector" : ".price"
                           }
                           
                           
 ], # rulePreconditions
 
 
 
 
 # Extract data from the table
 
 "ruleCSSSelector": "dl.property__details dd:nth-child(1)",
 
 "ruleTargetAttribute": "text",
 
 "ruleContentCondition": "",
 
 # Does this rule return more than one result?
 "ruleReturnsMore": False,
 
 # If the rule returns more than one result, which result to return. Negative means all elements. 
 # This takes only effect if ruleReturnsMore is set to True.
 "ruleReturnedMatchPos": 0,
 
 # NOT YET SUPPORTED. How strict should the extraction be? If rule returns more than one result, should this be considered
 # an error?
 "ruleReturningMoreIsError": False,
 
 # List of characters to remove from the extracted value (text or attribute)
 "ruleRemoveChars": [],
},




# Rule to extract/scrape links to listings FROM THE search result page

{

 # If no ruleName is specified, the extracted data of this rule cannot be used
 # in  the csvLineFormat attribute.
 
 "ruleName": "properties",
 
 
 # Short description
 
 "ruleDescription": "Extracts property data from search results page",
 
 
 
 # Regular expression specifying which URL pattern will trigger the
 # execution of this rule. 
 # This is a list ([]) meaning you may add many disjunctive regular expressions
 # Here we specify that this rule is to be activated when the URL contains en.wikipedia.org/wiki.
 # Special regular expression metacharacters (.) are escaped.
 
 "ruleURLActivationCondition": ["spitogatos\.gr/.*-katoikies.*/.*"],
 
 
 
 # A CSS selector specifying the element on the page to scrape.
 # NOTE: the CSS selector may return more than one mathing element.
 
 "ruleCSSSelector": ".ordered-element",
 
 
 # Once the CSS selector element in  ruleCSSSelector has been found, what 
 # exactly to extract from element: the text or some other attribute. 
 # text means simply return the text of the scraped element.
 "ruleTargetAttribute": "text",
 
 
 
 
 "rulePostCSSSelector" : ["h3:nth-child(1)", "h3:nth-child(2)", ".price__text"]
 "ruleReturnedValueNames" : ["propertyType", "propertyLocation", "propertyPrice"]
  
 
 # Regular expression that specifies the condition the extracted text or attribute value has to
 # fulfill. Empty string here means no condition. If condition is not met, nothing is returned.
 "ruleContentCondition": "",
 
 # Does this rule return more than one result?
 "ruleReturnsMore": True,
 
 # If the rule returns more than one result, which result to return. Negative means all elements. 
 # This takes only effect if ruleReturnsMore is set to True.
 "ruleReturnedMatchPos": 0,
 
 # NOT YET SUPPORTED. How strict should the extraction be? If rule returns more than one result, should this be considered
 # an error?
 "ruleReturningMoreIsError": False,
 
 # List of characters to remove from the extracted value (text or attribute)
 "ruleRemoveChars": [],
},







                        
 

# Rule for extracting links from the page
# A the name getLinks is a keyword signifying a rule that extracts
# hyperlinks found on the page what match the specified conditions.
#



{

 # Name of rule. Remember getLinks is a reserved word implying
 # a very specific behavior.
 "ruleName": "getLinks",
 
 "ruleDescription": "Extracting hyperlinks from the downloaded webpage",
 
 # Regular expression specifying for which URLs/pages to activate this rule.
 "ruleURLActivationCondition": ["spitogatos\.gr.*$"],
 
 # CSS selector containing the information (hyperlink) to extract
 "ruleCSSSelector": "a[href]",
 
 # From the specified selector, we are interested only in the value of the
 # attribute href, which contains the hyperlink.
 # NOTICE: WebScraper takes care of relative links
 "ruleTargetAttribute": "href",
 
 # Once we got the value of the target attribute, this regular expression
 # specifies a condition that the value has to meet. Here, we are interested 
 # only in hyperlinks that point to english wikipedia pages. I.e. pages outside
 # of the english wikipedia site will not be extracted and hence won't be followed.
 "ruleContentCondition": "spitogatos",
 
 # This signifies that this rule will not return only one chunk of information, but
 # a list of chunkc since a web page may have many <a href= > tags.
 "ruleReturnsMore": True,
 
 # -1 means return all matched
 "ruleReturnedMatchPos": -1,
 
 "ruleReturningMoreIsError": False 
}

]


}