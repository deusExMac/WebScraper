
########################################################################################################################################################
#
# Example library 9: Demonstrating the use of fields related to user agent and request cookies during retrieval of 
#                    resources.
#                                   
# 
# Related fields: requestCookies, requestUserAgent
#
#
#
# To set request cookies set the value of the field requestCookies. This field is a JSON object literal with zero or more key/value pairs, with 
# each key-value pair representing a cookie name and value respectively that will be sent to the server. If this object literal has two
# key-value pairs, this means that two cookies will be send with each request.
# Cookies are specified at the library level and the specified cookies are sent along with any request
# to the server. Currently, cookies cannot be specified per server; this means that the cookies specified in the field 
# requestCookies will be send along with every HTTP request independently of the server. This poses some serious issues related to
# compromising sensitive information. 
#
# To avoid sending cookies to unrelated servers/hosts/urls, make sure that the rules extract and follow only links within domains in 
# which the cookies are valid. This bug will be fixed in future versions.
#
# This example sets one cookie related to the users consenting accepting cookies when accesing youtube.com, without having signed in.
# The cookie that is set in this example named "CONSENT" has as a result that the consent form "Before you continue to YouTube" is not shown.
# This example sets only one cookie but more can be added; for example cookies related to session ids or authentication information can be added
# in this file in order to access youtube pages as a authenticated user. Such cookies can be retrieved by using the broser's (Mozilla, Chrome, Safari etc)
# developer tools, signing in to youtube and capture the desired cookies send with each request. These cookies (name and value) can be added here to the requestCookies
# field.
#
# 
#
# When to set requestCookies:
#      1) When cookies influences the page content that the server returns and you are interested scrapping from such pages.
#
# 
# The example has only the getLinks rule meaning that is downloads a page from youtube and extracts only the links leading to youtube.
#
# 
# How to use this library:
#
#  To see the impact of the CONSENT cookie, this library has to be executed 2 times: one time without specifying the request cookie and one with specifying
#  the request cookie. These different execurions will yield to different pages downloaded from youtube.
#
#  I) Execute library without CONSENT cookie
#     1) Go to field requestCookies below and comment out the line 
#         "CONSENT": "YES+cb.20211005-08-p0.en+FX+206"   
#        by putting a # before CONSENT
#     2) Open the WebScraper configuration file (default webscraper.conf) and make sure that option ignoreResponseCookies is set to True
#     3) Deploy this exr file by executing the following command: 
#
#            crawl -M -n 1 -r rules/example9-youtube.exr -o csv/example9.csv  https://www.youtube.com/
#
#        -M option is important as it will store all downloaded files on the disk, mirroring the remote website locally. Saved pages can be found in the directory
#        specified by mirrorRoot in for configuration (.conf) file.
#     4) When the download finishes, go to the directory <mirrorRoot>/www.youtube.com and open file index.html with a browser.
#     5) What you'll see is youtube's cookie consent form ("Before you continue to YouTube") that must be confirmed by the user.
#
#  II) Execute library with CONSENT cookie
#      1) Go to field requestCookies below and uncomment the line 
#          #"CONSENT": "YES+cb.20211005-08-p0.en+FX+206"
#         by removing the preceding # character if it is commented out. This means that during each request, the CONSENT cookie will now be send.
#      2) Deploy this exr file by executing the following command:
#
#             crawl -M -n 1 -r rules/example9-youtube.exr -o csv/example9.csv  https://www.youtube.com/
#
#         -M option is important as it will store all downloaded files on the disk, mirroring the remote website locally. Saved pages can be found in the directory
#         specified by mirrorRoot in for configuration (.conf) file.
#      3) When the download finishes, go to the directory <mirrorRoot>/www.youtube.com and open file index.html with a browser.
#      4) What you'll see is a different page than in case I) above where the consent form is now missing and you see the home page of youtube. This is because
#         the CONSENT cookie was sent with the request influencing the server's returned response.
# 
# v0.1@22/09/2022
#
########################################################################################################################################################



{

# Description of the library

"libraryDescription": "Library to use request cookies and user agent fetching the home page of youtube",



# List of rule names (or ruleReturnedValueNames as is the case in this example), whose extracted data should be stored in the csv file. 
# IMPORTANT: date the url was accessed (dateaccessed) and the url are always automatically added to the csv file

"csvLineFormat":[],

# The list of ruleNames (or ruleReturnedValueNames) that must return non-empty values
# to consider the extraction successful and the data be written to the csv file. 

"requiredFilledFields": [],

# Minimum percentage of ruleName that must return non-empty
# data during their application in order to consider the extraction process
# a success and hence add the extracted data to the csv file.

"allowedMinimumFilled" : 0.8


# How should the downloaded html page be rendered.
# WebScraper supports two modes of URL downloads:
# static: meaning that the web page does not load its content dynamically (via js or ajax) and one http request is enough to get
#         the entire page content. 
# dynamic: meaning the athe web page has dynamic content that is loaded via js or ajax once the web page has
#          been downloaded or is scrolled. Example of such dynamic pages are e.g. youtube pages where comments are only displayed
#          when the user scrolls down. Scraping such dynamic pages is also supported by WebScraper. Dynamic pages load slower though.
# 
# If renderPages has a value of False, this means the no page rendering is carried out and should be used only in case of pages that
# do not load content dynamicaly.
# If renderPages has a value of True, this means the page rendering is done and should be used only in case of pages that load content
# dynamically. 
# 
# Since we access wikipedia articles with no dynamic content, we will statically load these pages. Hence renderPages is set to False. This
# will make page loading faster. If renderPages is missing, it defaults to False.

"renderPages":False,




# Request cookies send with each request. 
# We sent here only one cookie, the cookie with name CONSENT and
# value YES+cb.20211005-08-p0.en+FX+206 in order to avoid receiving youtube's cookie consent form.
#
# You may add more cookies to send along with each request
# by adding key-value pairs in the form key:value where key is the cookie name
# and value the cookie's value. key-value pairs must be strings (enclosed in "") and if more
# key-value pairs (cookies) should be send, these must be separated by comma.
#

"requestCookies": {
                   "CONSENT": "YES+cb.20211005-08-p0.en+FX+206"                    
},


# User-Agent to use with all requests.
# If this field is empty or missing, the default (module) user-agent string
# will be used.

"requestUserAgent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0",





######################################################################################
#
#  List of rules that will be applied on every downloaded page starts here
#
######################################################################################




# List of individual rules comprising this library and that will be applied to each page downloaded follows. 
# This library consists only of the getLinks rule to extract hyperlinks. 

"library": [


{
 
 "ruleName": "getLinks",
 
 
 # Short description
 
 "ruleDescription": "Extracts links landing on youtube pages",
 
 
 # For which URLs to activate this rule
 
 "ruleURLActivationCondition": ["youtube\.com"],
 
 
 # A CSS selector specifying the element on the page to scrape.
 # NOTE: the CSS selector may return more than one mathing element.
 
 "ruleCSSSelector": "a[href]",
 
 
 
 
 # Once the CSS selector element in  ruleCSSSelector has been found, or data has been
 # extracted by ecRuleCSSSelector, what 
 # exactly to extract from this element: the text or some other attribute. 
 # text means simply return the text of the scraped element.
 #
 # In ruleMatchPreconditions, ruleTargetAttribute will be applied to the newly extracted
 # elements.
 
 "ruleTargetAttribute": "href",
 
 
 
 # Regular expression that specifies the condition the extracted text or attribute value has to
 # fulfill. Empty string here means no condition. If condition is not met, nothing is returned.
 #
 # We require links to land on youtube pages in order to solve the cookie issue that exists in
 # this version (see introduction above).
 
 "ruleContentCondition": "youtube\.com",
 
 
 # Does this rule return more than one result?
 "ruleReturnsMore": True,
 
 # If the rule returns more than one result, which result to return. Negative means all elements. 
 # This takes only effect if ruleReturnsMore is set to True.
 "ruleReturnedMatchPos": -1,
 
 # NOT YET SUPPORTED. How strict should the extraction be? If rule returns more than one result, should this be considered
 # an error?
 "ruleReturningMoreIsError": False,
 
 # List of characters to remove from the extracted value (text or attribute)
 "ruleRemoveChars": [],

} # getLinks


] # end of list of rules


} # End of json file