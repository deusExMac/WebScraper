
########################################################################################################################################################
#
# Example library 8.4: Demonstrating how to interact with web pages. Demonstrates how to click on buttons and submitting forms executied in sequence.
#                    
# 
# Related fields: ruleDynamicElements, dpcType, dpcPageElement, dpcScrolldown, dpcWaitFor, dpcFillContent, renderPages 
#
#
# TODO: Update next paragraph. CURRENTLY WRONG DESCRIPTION!!!! INCOMPLETE DESCRIPTION
#
# This example extracts from a table captioned "Performances in the European Cup and UEFA Champions League by club" 
# found on the page
#
#   https://en.wikipedia.org/wiki/List_of_European_Cup_and_UEFA_Champions_League_finals
#
# the name of the teams, the times they won the Champions League, the times they were runner-ups as well as
# the seasons they won the Champions League and it does this only for those teams that
# have won the champions league title more than 3 times. Applying this set of rules will return more than one 
# matching records (i.e. a recordlist) when applied to the above page. Since from each match a new set of elements
# is extracted with new key names, the fields csvLineFormat and requiredFilledFields need to be be updated
# with the new key names.
# 
#
# When to set renderPages to true:
#      1) When the page loads its html content dynamically, when user interacts with the page e.g. by 
#         scrolling down (see e.g. youtube comments)
#      2) When the page requires some form of interaction in term of button or element clicks by the user
#      3) When elements (besides the entire page) needs to be scrolled in order to load their content.
#
# TODO: Update next paragraphs.
#
# Aplying rulePostCSSSelector will return a list of dictionaries known as a recordlist. Examples up to this point
# returned simple records, i.e. list of key/value pairs where the value is a simple value or a list of string valued.
# Each dict is saved as separate line in the csv file.
# To save these dictionaries to the csv file, the fields csvLineFormat and requiredFilledFields need to be updated properly with the new
# fields.
#
# 
#
# The example rule below extracts the name of ALL football teams found on the page below
# that have won the Champions league more than three times (>3) in their club history.
# It does this by downloading the wikipedia page 
#
#    https://en.wikipedia.org/wiki/List_of_European_Cup_and_UEFA_Champions_League_finals
#
# and extracting data from the table with caption "Performances in the European Cup and UEFA Champions League by club".
#
# To achieve this, the rule does the following once the above wikipedia page has been downloaded:
#
# 1) Extracts all table rows of a specific table (tr) containing club names and the times they have won the Champions League
# 2) Applies a match precondition on EACH tr element extracted in 1) that checks i) if a specific td, containing data on the number of times a 
#    team has won, has value greater than 3 and ii) if i) holds, it returns the name of the team which is in a different part (a th) of the same table row (tr) by applying
#    ecRuleCSSSelector on the tr element currently processed. 
#
# More than one match preconditions can be defined in the context of the same rule. Currently, only operator ANY is supported for these match preconditions.
# If multiple match preconditions match, the first matching precondition's ecRuleCSSSelector is applied, if specified.
# 
# How to use this rule (NOTE: THIS IS CORRECT):
#
# crawl -n 1 -r rules/example8-en.wikipedia.exr -o csv/example8.csv  https://en.wikipedia.org/wiki/List_of_European_Cup_and_UEFA_Champions_League_finals
#
#
# 
# v0.1@09/09/2022
#
########################################################################################################################################################



{

# Description of the library

"libraryDescription": "Library to submit forms by clicking on submit button.",



# List of rule names (or ruleReturnedValueNames as is the case in this example), whose extracted data should be stored in the csv file. 
# IMPORTANT: date the url was accessed (dateaccessed) and the url are always automatically added to the csv file

"csvLineFormat":[],

# The list of ruleNames (or ruleReturnedValueNames) that must return non-empty values
# to consider the extraction successful and the data be written to the csv file. 

"requiredFilledFields": [],

# Minimum percentage of ruleName that must return non-empty
# data during their application in order to consider the extraction process
# a success and hence add the extracted data to the csv file.

"allowedMinimumFilled" : 0.8


# How should the downloaded html page be rendered.
# WebScraper supports two modes of URL downloads:
# static: meaning that the web page does not load its content dynamically (via js or ajax) and one http request is enough to get
#         the entire page content. 
# dynamic: meaning the athe web page has dynamic content that is loaded via js or ajax once the web page has
#          been downloaded or is scrolled. Example of such dynamic pages are e.g. youtube pages where comments are only displayed
#          when the user scrolls down. Scraping such dynamic pages is also supported by WebScraper. Dynamic pages load slower though.
# 
# If renderPages has a value of False, this means the no page rendering is carried out and should be used only in case of pages that
# do not load content dynamicaly.
# If renderPages has a value of True, this means the page rendering is done and should be used only in case of pages that load content
# dynamically. 
# 
# Since we access wikipedia articles with no dynamic content, we will statically load these pages. Hence renderPages is set to False. This
# will make page loading faster. If renderPages is missing, it defaults to False.

"renderPages":True,

"requestCookies": {
                                      
},

# ruleDynamicElements BELOW ARE correct for: Clicking on google consent page, filling input with python 
# clicking on submit button are correct.
#
# THE AIM IS TO BE ABLE TO search on google. ASSUMES access to google page is without authentication cookies
# i.e. as a guest.

"ruleDynamicElements": [ 

         # This rule is to get rid of 
         # the google consent page 
         # by clicking on the button id L2AGLb.
         # If an authentication cookie is set, the next is not 
         # required
         {
		     "dpcURLActivationCondition" : "google\.com/?$",
		     "dpcType":"click",
		     "dpcPageElement":"#L2AGLb",
		     "dpcScrolldown":-1,
		     "dpcWaitFor":"img.lnXdpd",
		     "dpcFillContent": ""		     
		 },
		 
         # Fill input box with search string. 
         # Here, python
		 {
		     "dpcURLActivationCondition" : "google\.com/?$",
		     "dpcType":"fill",
		     "dpcPageElement":"input.gLFyf",
		     "dpcScrolldown":-1,
		     "dpcWaitFor":"",
		     "dpcFillContent": "python"		     
		 },
		 
         # Click the search button.
		 {
		     "dpcURLActivationCondition" : "google\.com/?$",
		     "dpcType":"click",
		     "dpcPageElement":"input[name='btnK']",
		     "dpcScrolldown":-1,
		     "dpcWaitFor":"#logo",
		     "dpcFillContent": "",
		     "dpcRedirects": False		     
		 },
		 

		 
],




######################################################################################
#
#  List of rules that will be applied on every downloaded page starts here
#
######################################################################################




# List of individual rules comprising this library and that will be applied to each page downloaded follows. 
# This library consists of one rule only, named teamNamesWithMoreThan3UCLTitles. 

"library": [



# Rule to extract/scrape the title of the downloaded article. The title
# is assumed to be the teams name, not it's full name (or formal name)

{

 # If no ruleName is specified, the extracted data of this rule cannot be used
 # in  the csvLineFormat attribute.
 
 "ruleName": "getLinks",
 
 
 # Short description
 
 "ruleDescription": "Extracts the team name that has won the UCL more than 2 times",
 
 
 
 # Regular expression specifying which URL pattern will trigger the
 # execution of this rule. 
 # This is a list ([]) meaning you may add many disjunctive regular expressions
 # Here we specify that this rule is to be activated when the URL contains en.wikipedia.org/wiki.
 # Special regular expression metacharacters (.) are escaped.
 
 "ruleURLActivationCondition": ["google\.com"],
 
 
 # A CSS selector specifying the element on the page to scrape.
 # NOTE: the CSS selector may return more than one mathing element.
 
 "ruleCSSSelector": "a[href]",
 
 # List of match preconditions that will be applied
 # to each and every result returned by ruleCSSSelector. Elements
 # matching preconditions will be kept in result list, elements that
 # don't match will be removed from the result set. The match precondition
 # in this example specifies the ecRuleCSSSelector field which means that
 # if the precondition holds for an element, a different part of the examined element is 
 # extracted and returned. 
 # If more than one match precondition is specified, these are
 # combined using ANY operator.
 # Currently, only ANY operator is supported.
 #
 # NOTE: Match preconditions without ecRuleCSSSelector field have not been tested.
 
 
 
 # Once the CSS selector element in  ruleCSSSelector has been found, or data has been
 # extracted by ecRuleCSSSelector, what 
 # exactly to extract from this element: the text or some other attribute. 
 # text means simply return the text of the scraped element.
 #
 # In ruleMatchPreconditions, ruleTargetAttribute will be applied to the newly extracted
 # elements.
 
 "ruleTargetAttribute": "href",
 
 
 
 
 # List of css selectors to apply to each extracted element by ruleCSSSelector.
 # From each extracted element, extract th and the td elements specified.
 # NOTE: It is assumed tha each of the selectors in rulePostCSSSelector, 
 # will return at most one element. 
 
 #"rulePostCSSSelector" : ["th", "td:nth-child(2)", "td:nth-child(3)", "td:nth-child(4)"]
 
 
 # These will be the key names under which the extracted elements by
 # rulePostCSSSelector will be stored. Name must be ordered based on selectors 
 # specified in rulePostCSSSelector.
 # NOTE: number of keys must be the same as number of css selectors in
 # rulePostCSSSelector.
 
 #"ruleReturnedValueNames" : ["name", "winnerCount", "runnerUpCount", "seasonsWon"]
 
 
 
 
 # Regular expression that specifies the condition the extracted text or attribute value has to
 # fulfill. Empty string here means no condition. If condition is not met, nothing is returned.
 "ruleContentCondition": "",
 
 # Does this rule return more than one result?
 "ruleReturnsMore": True,
 
 # If the rule returns more than one result, which result to return. Negative means all elements. 
 # This takes only effect if ruleReturnsMore is set to True.
 "ruleReturnedMatchPos": -1,
 
 # NOT YET SUPPORTED. How strict should the extraction be? If rule returns more than one result, should this be considered
 # an error?
 "ruleReturningMoreIsError": False,
 
 # List of characters to remove from the extracted value (text or attribute)
 "ruleRemoveChars": [],
}


]


}