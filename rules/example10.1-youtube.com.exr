
########################################################################################################################################################
#
# Example library 10.1: Demonstrating interacting with the downloaded web page to load dynamically its content. In this 
#                       example, one simple interaction is shown namely scrolling of the web page in order to load 
#                       more of its content before extracting/scraping data. Interacting with web pages requires
#                       the pages to be loaded using a browser rendering engine i.e. renderPages to be set to True.
#                    
# 
# Related fields: renderPages, ruleDynamicElements, dpcURLActivationCondition, dpcType, dpcScrolldown 
#
# This example extracts the names of users that have commented on youtube videos. A youtube page, when loaded
# does not immediately load the comment section beneath the video. On youtube in order for the video comments to appear/load,
# the page must be scrolled down. The more the page is scrolled down, the more comments will appear. This library
# shows how to scroll page and configure some aspects of scrolling process.
#
# To define an interaction inside an .exr file, the field ruleDynamicElements must be used which consists of
# a list of interactions that should be applied on the loaded page. The range of interactions includes, scrolling the page,
# filling input elements, clicking buttons etc. In this example, only one iteraction if defined, that scrolls down
# the web page thereby loading more youtube comments. If more than one interactions is present, these all are executed  
# on the downloaded page, if constraints match, and in the order they are defined. 
#
# This example demonstrates also how to control the number of times the web page should be scrolled down. Here, a constant, fixed amount of
# scrolls are applied to all youtube pages.
#
# 
#
# IMPORTANT: This example makes the assumption that no authentication cookie is used for the requst i.e. access to youtube pages is done as a 
#            non-authenticated account. When authentication cookie is present, selectors might need to change.
# 
# How to apply this rule:
#
# v0.05a){1}WebScraper >> crawl -n 1 -r rules/example10.1-youtube.com.exr -o csv/example10.1.csv https://www.youtube.com/watch?v=GJDNkVDGM_s
#
# This will extract the name of users that have commented on the youtube video. It wi;; extract the user names commented on one video. Change
# the -n argument appropriately if you would like to extract user names as WebScraper crawls youtube pages.
# 
# v0.1@18/09/2022
#
########################################################################################################################################################



{

# Description of the library

"libraryDescription": "Library to extract the user names from comments beneath videos on youtube pages.",



# List of rule names, whose results should be stored in the csv file. Here only articleTitle is
# mentioned meaning that only the result of the rule named articleTitle should be stored in the csv file.
# If a rule name is NOT included in this list, the data extracted by that rule is NOT stored in the csv file.
# IMPORTANT: date the url was accessed (dateaccessed) and the url are always automatically added to the csv file

"csvLineFormat":["userName"],



# How should the downloaded html page be downloaded? Here, activation of a browser page rendering 
# engine is activated.
# WebScraper supports two modes of URL downloads:
# static: meaning that the web page does not load its content dynamically (via js or ajax) and one http request is enough to get
#         the entire page content. 
# dynamic: meaning the athe web page has dynamic content that is loaded via js or ajax once the web page has
#          been downloaded or is scrolled. Example of such dynamic pages are e.g. youtube pages where comments are only displayed
#          when the user scrolls down. Scraping such dynamic pages is also supported by WebScraper. Dynamic pages load slower though.
# 
# If renderPages has a value of False, this means that pages are downloaded using http requests and no page rendering is carried out.
# If renderPages has a value of True, this means the pages are downloaded using a page rendering engine used by browsers. 
# 
# Since we access wikipedia articles with no dynamic content, we will statically load these pages. Hence renderPages is set to False. This
# will make page loading faster. If renderPages is missing, it defaults to False.

"renderPages":True,


# ruleDynamicElements is a field that defines the set of interactions with a loaded web pages, i.e. 
# operations that can be applied n a web page after it has been loaded. These operations/interactions
# are applied on the page after the page has been loaded and before the data extraction process
# i.e. application of rules begins.
#  +++++++ TODO +++++
# BELOW ARE correct for: Clicking on google consent page, filling text with python 
# clicking on submit button are correct.
#
# THE AIM IS TO BE ABLE TO search on google

"ruleDynamicElements": [ 

         
         {
		     "dpcURLActivationCondition": "youtube\.com",
		     "dpcType":"scrollpage",
		     "dpcPageElement":"",
		     "dpcScrolldown":-1,
		     
		     # dpcScrollTargetElementCount encodes condition to stop PAGE scrolling
		     # based on the number of desired elements.
		     #
		     # scrollTargetSelector: CSS Selector of items to count
		     # scrollTargetCount: how many elements specified by  scrollTargetSelector should
		     # be on the page before stop scrolling
		     # IMPORTANT: scrollTargetSelector and scrollTargetCount keys are mandantory.
		     # scrollTargetCount is an integet, but here MUST be encoded as a string
		     # 
		     "dpcScrollTargetElementCount" : { "scrollTargetSelector": "#author-text .ytd-comment-renderer", "scrollTargetCount":"71" },
		     
		     
		     
		     "dpcWaitFor":"",
		     "dpcFillContent": ""		     
		 }
],





# Request cookies send with each request. 
# We sent here only one cookie, the cookie with name CONSENT and
# value YES+cb.20211005-08-p0.en+FX+206 in order to avoid receiving youtube's cookie consent form
# since we access a youtube video page without identifying first.
#

"requestCookies": {
                   "CONSENT": "YES+cb.20211005-08-p0.en+FX+206"                    
},




######################################################################################
#
#  List of rules that will be applied to every downloaded page starts here.
#
#  Rules are grouped into a library. All rules will be applied to the 
#  downloaded page if some rule-specific conditions are met. If rule-specific
#  conditions are not met, the rule is not applied. 
#  Each rule extracts specific data or list of data from the (same) page. 
#
######################################################################################



# List of individual rules comprising this library and that will be applied to each page downloaded follows. 
# This library consists of one rule only, named articleTitle. The value extracted by this rule
# will be assigned to a special key/variable with the same name as the rule i.e. articleTitle.
# These keys/variables can be references inside thi rule file e.g. see csvLineFormat field above.

"library": [

{
 # Rules must have a name, since these ruleNames will be the names of keys to store
 # the extracted values by that rule. rules names must be unique in the context
 # of the same library.
 # Rule names can not contain the following characters: whitespace, -
 
 "ruleName": "userName",
 
 
 # ruleDescription are optional. Helpful but optional.
 
 "ruleDescription": "Extracts the user name of users commenting on youtube videos",
 
 
 
 # Regular expression specifying which URL pattern will trigger the
 # execution of this rule. 
 # This is a list ([]) meaning you may add many disjunctive regular expressions
 # Here we specify that this rule is to be activated when the URL contains en.wikipedia.org/wiki.
 # Special regular expression metacharacters (.) are escaped.
 "ruleURLActivationCondition": ["youtube\.com/watch"],
 
 # A CSS selector specifying the element on the page to scrape.
 # NOTE: the CSS selector may return more than one mathing element.
 "ruleCSSSelector": "#author-text .ytd-comment-renderer",
 
 # Once the CSS selector element in  ruleCSSSelector has been found, what 
 # exactly to extract from element: the text or some other attribute. 
 # text means simply return the text of the scraped element.
 "ruleTargetAttribute": "text",
 
 # Regular expression that specifies the condition the extracted text or attribute value has to
 # fulfill. Empty string here means no condition. If condition is not met, nothing is returned.
 "ruleContentCondition": "",
 
 # Does this rule return more than one result from a single page??
 "ruleReturnsMore": True,
 
 # If the rule returns more than one result, which result to return. Negative means all elements. 
 # This takes only effect if ruleReturnsMore is set to True.
 "ruleReturnedMatchPos": -1,
 
 # NOT YET SUPPORTED. How strict should the extraction be? If rule returns more than one result, should this be considered
 # an error?
 "ruleReturningMoreIsError": False,
 
 # List of characters to remove from the extracted value (text or attribute)
 "ruleRemoveChars": [],
},






# Rule for extracting links from the page
# A the name getLinks is a keyword signifying a rule that extracts
# hyperlinks found on the page what match the specified conditions.
#
# When you would like to extract and follow hyperlinks found on a webpage,
# you must name the respective rule extracting these hyperlins getLinks. 
# This is because the extracted links are handled differently from other 
# extracted data (e.g. they must be added to the queue).


{

 # Name of rule. Remember getLinks is a reserved word implying
 # a very specific behavior.
 "ruleName": "getLinks",
 
 "ruleDescription": "Extracting hyperlinks from the downloaded webpage",
 
 # Regular expression specifying for which URLs/pages to activate this rule.
 "ruleURLActivationCondition": ["en\.wikipedia\.org.*$"],
 
 # CSS selector containing the information (hyperlink) to extract
 "ruleCSSSelector": "a[href]",
 
 # From the specified selector, we are interested only in the value of the
 # attribute href, which contains the hyperlink.
 # NOTICE: WebScraper takes care of relative links
 "ruleTargetAttribute": "href",
 
 # Once we got the value of the target attribute, this regular expression
 # specifies a condition that the value has to meet. Here, we are interested 
 # only in hyperlinks that point to english wikipedia pages. I.e. pages outside
 # of the english wikipedia site will not be extracted and hence won't be followed.
 "ruleContentCondition": "en\.wikipedia\.org/.*$",
 
 # This signifies that this rule will not return only one chunk of information, but
 # a list of chunkc since a web page may have many <a href= > tags.
 "ruleReturnsMore": True,
 
 # -1 means return all matched
 "ruleReturnedMatchPos": -1,
 
 "ruleReturningMoreIsError": False 
}

]


}