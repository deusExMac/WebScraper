
########################################################################################################################################################
#
# Example library 3: Demonstrating application of three rules on downloaded page extracting different parts. One of 
# these rules extracts hyperlinks. This rule is always named getLinks. This is a reserved word.
# 
# This library scraps/extracts the team's name and team's full name (formal name) of football club pages in wikipedia. For example,
# for the football team Napoli, it's name is S.S.C. Napoli but its full name is "Societa Sportiva Calcio Napoli S.p.A." as can be
# seen on its wikipedia page https://en.wikipedia.org/wiki/S.S.C._Napoli 
#
# The library contains also the special rule named "getLinks" for harvesting links.
# 
# How to apply this rule:
#
# v0.05a){1}WebScraper >> crawl  -r rules/example3-en.wikipedia.exr -o csv/example3.csv https://en.wikipedia.org/wiki/List_of_football_clubs_in_Italy
#
# This will extract the team's name (title) and full name of all the wikipedia pages downloaded and store it in the csv file named example3.csv .
# NOTE: You may find in the .csv file entries with empty values for the teams full name. This is because this library does not make any check or 
# does not imply any precondition before applying the rules.
# 
# v0.1@5/8/2022
#
########################################################################################################################################################


{

# Description of the library

"libraryDescription": "Library to extract the team name, team full name and hyperlinks leading to wikipedia from wikipedia articles related to football clubs",



# List of rule names, whose results should be stored in the csv file. Here only articleTitle is
# mentioned meaning that only the result of the rule named articleTitle should be stored in the csv file.
# IMPORTANT: date the url was accessed (dateaccessed) and the url are always automatically added to the csv file

"csvLineFormat":["teamName", "teamFullName"],



# How should the downloaded html page be rendered.
# WebScraper supports two modes of URL downloads:
# static: meaning that the web page does not load its content dynamically (via js or ajax) and one http request is enough to get
#         the entire page content. 
# dynamic: meaning the athe web page has dynamic content that is loaded via js or ajax once the web page has
#          been downloaded or is scrolled. Example of such dynamic pages are e.g. youtube pages where comments are only displayed
#          when the user scrolls down. Scraping such dynamic pages is also supported by WebScraper. Dynamic pages load slower though.
# 
# If renderPages has a value of False, this means the no page rendering is carried out and should be used only in case of pages that
# do not load content dynamicaly.
# If renderPages has a value of True, this means the page rendering is done and should be used only in case of pages that load content
# dynamically. 
# 
# Since we access wikipedia articles with no dynamic content, we will statically load these pages. Hence renderPages is set to False. This
# will make page loading faster. If renderPages is missing, it defaults to False.

"renderPages":False,



# List of individual rules comprising this library and that will be applied to each page downloaded follows. 
# This library consists of one rule only, named articleTitle. The value extracted by this rule
# will be assigned to a special key/variable with the same name as the rule i.e. articleTitle.
# These keys/variables can be references inside thi rule file e.g. see csvLineFormat field above.

"library": [


# Rule to extract/scrape the title of the downloaded article. The title
# is assumed to be the teams name, not it's full name (or formal name)

{
 "ruleName": "teamName",
 "ruleDescription": "Extracts the title of a wikipedia article",
 
 # Regular expression specifying which URL pattern will trigger the
 # execution of this rule. 
 # This is a list ([]) meaning you may add many disjunctive regular expressions
 # Here we specify that this rule is to be activated when the URL contains en.wikipedia.org/wiki.
 # Special regular expression metacharacters (.) are escaped.
 "ruleURLActivationCondition": ["en\.wikipedia\.org/wiki"],
 
 # A CSS selector specifying the element on the page to scrape.
 # NOTE: the CSS selector may return more than one mathing element.
 "ruleCSSSelector": "#firstHeading",
 
 # Once the CSS selector element in  ruleCSSSelector has been found, what 
 # exactly to extract from element: the text or some other attribute. 
 # text means simply return the text of the scraped element.
 "ruleTargetAttribute": "text",
 
 # Regular expression that specifies the condition the extracted text or attribute value has to
 # fulfill. Empty string here means no condition. If condition is not met, nothing is returned.
 "ruleContentCondition": "",
 
 # Does this rule return more than one result?
 "ruleReturnsMore": False,
 
 # If the rule returns more than one result, which result to return. Negative means all elements. 
 # This takes only effect if ruleReturnsMore is set to True.
 "ruleReturnedMatchPos": 0,
 
 # NOT YET SUPPORTED. How strict should the extraction be? If rule returns more than one result, should this be considered
 # an error?
 "ruleReturningMoreIsError": False,
 
 # List of characters to remove from the extracted value (text or attribute)
 "ruleRemoveChars": [],
},


#
# This rule returns a specific field in the infobox of wikipedia articles related to fottball clubs
# that contains the teams' formal name.
#
{
 "ruleName": "teamFullName",
 "ruleDescription": "Extracts the team's full name from the team's wikipedia page. A teams full name is found in a infobox, in sencond row (below the crest) next to Full name e.g. see https://en.wikipedia.org/wiki/Juventus_F.C. ",
 
 # Regular expression specifying which URL pattern will trigger the
 # execution of this rule. 
 # This is a list ([]) meaning you may add many disjunctive regular expressions
 # Here we specify that this rule is to be activated when the URL contains en.wikipedia.org/wiki.
 # Special regular expression metacharacters (.) are escaped.
 "ruleURLActivationCondition": ["en\.wikipedia\.org/wiki"],
 
 # A CSS selector specifying the element on the page to scrape.
 # We assume that the full name can be found in this page element.
 #
 # If this rule is applied to a wikipedia page that is not about a football club,
 # the CSS selector may not be valid and an empty string is returned. You may see in the .csv file
 # empty values for field teamFullName.
 # This is because there are no conditions specified when to apply this selector. ruleCSSSelectors are always
 # applied on pages, without any checking whther or not these exists.
 #
 "ruleCSSSelector": "tr:nth-child(2) .infobox-data",
 
 # Once the CSS selector element in  ruleCSSSelector has been found, what 
 # exactly to extract from element: the text or some other attribute. 
 # text means simply return the text of the scraped element.
 "ruleTargetAttribute": "text",
 
 # Regular expression that specifies the condition the extracted text or attribute value has to
 # fulfill. Empty string here means no condition. If condition is not met, nothing is returned.
 "ruleContentCondition": "",
 
 # Does this rule return more than one result?
 "ruleReturnsMore": False,
 
 # If the rule returns more than one result, which result to return. Negative means all elements. 
 # This takes only effect if ruleReturnsMore is set to True.
 "ruleReturnedMatchPos": 0,
 
 # NOT YET SUPPORTED. How strict should the extraction be? If rule returns more than one result, should this be considered
 # an error?
 "ruleReturningMoreIsError": False,
 
 # List of characters to remove from the extracted value (text or attribute)
 "ruleRemoveChars": [],
},





# Rule for extracting the hyperlinks from the page
# A the name getLinks is a keyword signifying a rule that extracts
# hyperlinks found on the page what match the specified conditions.
#
# When you would like to extract and follow hyperlinks found on a webpage,
# you must name the respective rule extracting these hyperlins getLinks. 
# This is because the extracted links are handled differently from other 
# extracted data (e.g. they must be added to the queue).


{

 # Name of rule. Remember getLinks is a reserved word implying
 # a very specific behavior.
 "ruleName": "getLinks",
 
 "ruleDescription": "Extracting hyperlinks from the downloaded webpage",
 
 # Regular expression specifying for which URLs/pages to activate this rule.
 # Here, only hyperlinks from english wikipedia articles are extracted.
 "ruleURLActivationCondition": ["en\.wikipedia\.org.*$"],
 
 # CSS selector containing the information (hyperlink) to extract
 "ruleCSSSelector": "a[href]",
 
 # From the specified selector, we are interested only in the value of the
 # attribute href, which contains the hyperlink.
 # NOTICE: WebScraper takes care of relative links
 "ruleTargetAttribute": "href",
 
 # Once we got the value of the target attribute, this regular expression
 # specifies a condition that the value has to meet. Here, we are interested 
 # only in hyperlinks that point to english wikipedia pages. I.e. pages outside
 # of the english wikipedia site will not be extracted and hence won't be followed.
 
 "ruleContentCondition": "en\.wikipedia\.org/.*$",
 
 # This signifies that this rule will not return only one chunk of information, but
 # a list of chunkc since a web page may have many <a href= > tags.
 "ruleReturnsMore": True,
 
 # -1 means return all matched
 "ruleReturnedMatchPos": -1,
 
 "ruleReturningMoreIsError": False 
}

]


}