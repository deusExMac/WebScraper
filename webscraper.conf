[Default]

outputDir = ./articles

; maximum number of articles to download
; set to -1 for unlimited
maxArticles = 12

; Should we download also the content of
; articles?
contentDownload = true


; Store documents in a csv file? If so, how?
csvSave = true
csvFileName = allArticles.csv
csvSeparator = ;




[Shell]
commandPrompt = WebScraper >>
historySize = 25



[Rules]

; File containing metadata for 
; extraction rules from pages
ruleFile = ./default.exr




[Crawler]

; Not yet used
httpUserAgent = 

; Maximum number of pages to fetch.
; Negative number means no limit.
maxPages = 10


; Two values supported: constant (c) and human (h)
delayModel = h

; Sleep time between consecutive http requests if you want constant delays between requests
; to the SAME server
; In seconds
sleepTime = 2.4

; When opting to select a kind of more "human behavior" of the 
; webscraper, the following determine a random sleep time (delay)
; between consecutive requests to the same server, drawn from
; a normal distribution with the below average and standard deviation (sigma)
; NOTE: In literature, avg time spend on a page for a human is about 12seconds
humanSleepTimeAvg = 12.4
humanSleepTimeSigma = 4.136




[Storage]

; Directory to mirror remote site
; Due to a bug, this must NOT end with a slash /
; May also have empty value
mirrorRoot = etc


