[Default]

outputDir = ./articles

; maximum number of articles to download
; set to -1 for unlimited
maxArticles = 12

; Should we download also the content of
; articles?
contentDownload = true


; Store documents in a csv file? If so, how?
csvSave = true
csvFileName = allArticles.csv
csvSeparator = ;




[Shell]
commandPrompt = WebScraper >>
historySize = 400



[Rules]

; File containing metadata for 
; extraction rules from pages
ruleFile = ./default.exr




[Crawler]

; How to traverse links. Two options supported bfs and dfs
; bfs for breadth first search and dfs for depth first search.
traversalStrategy = dfs

; Not yet used
httpUserAgent = 

; Maximum number of pages to fetch.
; Negative number means no limit.
; Can be overwritten by command line arguments
maxPages = 10




; Should response cookies that are not
; in the existing list of cookies be ignored or added to the existing list? 
; If this is set to False, then any new
; response cookies are added to existing ones and used
; in subsequent requests to the server. 
ignoreResponseCookies = False

; List of response cookie names to ignore i.e. not
; to add to existing resuest cookies.
; This option works only if ignoreResponseCookies is set to False
; Values: comma separated cookie-names. Empty value means no
; cookie is ignored, if ignoreResponseCookies is se to False.
ignoredCookies = 


; Should we take and save a screenshot of the page
; downloaded?
; Has effect ONLY if fetch mode is dynamic i.e. renderPages in exr
; file is True. Screeshots will be save in path specified by
; screenShotPath in this config file.
takePageScreenShot = True


; How much time to wait after an asynchronous
; call to load a page or perform an operation on 
; a page.
; waitTime takes effect only if fetch method is dynamic, i.e.
; renderPages is set to true.
asyncWaitTime = 1.5 


; Minimum hit rate accepted. 
; Hit rate = percentage of pages from which valid data have been extracted.
; if hit rate falls below this limit,
; crawling stops. Negative values disable hit rate
minHitRate = 0.023

; Number of consecutive hit rate samples have to be
; below minHitRate before terminating
minHitRateSamples = 121


;
; Allowed maximum number of URLs in URL queue (keeping the to-be-visited URLs). If this threshold is reached,
; new URLs are not added to the queue. URL queue will never exceed this
; number.
; -1 means no limit.
maxQueueSize = -1

;
; Allowed maximum memory size of the url queue.
; If this size is reached, no new urls are added to the queue.
; Negative means no limit.
; May specify K, M, G at the end for units e.g. 8M for 8 Megabyte. If no unit
; is specified, number is interpreted as bytes.
; 
maxQueueMemorySize = -1

; Two values supported: constant (c - meaning same delay between requests) and human (h)
delayModel = h

; Sleep time between consecutive http requests if you want constant delays between requests
; to the SAME server
; In seconds
sleepTime = 2.4

; When opting to select a kind of more "human behavior" of the 
; webscraper, the following determine a random sleep time (delay)
; between consecutive requests to the same server, drawn from
; a normal distribution with the below average and standard deviation (sigma)
; NOTE: In literature, avg time spend on a page for a human is about 12seconds
humanSleepTimeAvg = 12.4
humanSleepTimeSigma = 4.136


; 
; Specifies if url queue and csv file (that are maintained in
; memory) should be stored in specified files periodically.
; TODO: not yet used
autoSave = True

; Interval - integer value. Expresses time in seconds
autoSaveInterval = 131

; maximum number of (t)ime (p)er (p)age samples to keep (i.e. time to process one page)
; out of which the average speed in pages/seconds is calculated
maxTPPSamples = 150



[Storage]

; Directory to mirror remote site
; Due to a bug, this must NOT end with a slash /
; May also have empty value
mirrorRoot = etc

; Where to store screenshots
screenShotPath = etc/sshots



[DEBUG]

; Accepted values True or False
debugging = False

